{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a comprehensive approach to predict the price of a house is taken.\n",
    "\n",
    "Approach taken:\n",
    "\n",
    "* Initial Data Exploration\n",
    "* Exploratory Data Analysis\n",
    "* Data Cleaning\n",
    "    - Handling missing values\n",
    "* Visualizing data distributtions and relationships\n",
    "* Handling Outliers\n",
    "* Handling Categorical Variables\n",
    "* Feature Selection\n",
    "* Model Training\n",
    "    - Splitting Dataset\n",
    "    - Hyperparameter Tuning\n",
    "    - Fitting model with best parameters\n",
    "    - Predicting on testing data\n",
    "    - Evaluating and comparing performance of different models\n",
    "\n",
    "In the modelling stage, I trained several machine learning algorithms, Logistic Regression, XGBoost Regressor, and Random Forest Regressor, for which I also performed hyperparameter tuning.\n",
    "\n",
    "Moreover, I experimented with the CatBoost Regressor, which naturally handles categorical data. \n",
    "\n",
    "It's important to note that while this comprehensive approach generally aids in creating robust models, the small size of the dataset, limited to 1460 entries, might have impacted the overall performance of the models.\n",
    "\n",
    "Overall, this project served as an excellent application of various data science concepts and provided valuable insights into the workings of different machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from category_encoders import HashingEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and reading training dataset as pandas dataframe\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 5 rows of the dataframe\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimension of the dataframe\n",
    "rows, cols = df.shape[0], df.shape[1]\n",
    "print(f\"There are {rows} rows and {cols} columns in the dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting overview of the dataset\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical details of the dataset (numerical variables only)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns are not included in the statistical summary provided by df.describe(). But we can get a summary of the categorical variables by passing only the columns that are categorical to describe() like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting statistical details of categorical variables\n",
    "cat_vars = df.dtypes[df.dtypes == \"object\"].index\n",
    "df[cat_vars].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am dropping the columns that are missing a lot of data as it is better to do so instead of filling it with mean, median or something else because it might mislead. Here, I am using 30% threshold which means dropping columns with more than 30% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 30% missing values\n",
    "threshold = 0.3  \n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentages = df.isnull().mean()\n",
    "\n",
    "# Get the list of columns to drop\n",
    "columns_to_drop = missing_percentages[missing_percentages > threshold].index\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "df_dropped = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop the 'Id' column \n",
    "df_dropped = df_dropped.drop(['Id'], axis=1)\n",
    "\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical and numerical columns\n",
    "cat_columns = df_dropped.select_dtypes(include=['object']).columns\n",
    "num_columns = df_dropped.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(f\"There are {len(cat_columns)} categorical variables.\")\n",
    "print(f\"There are {len(num_columns)} numerical variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out which of the categorical and numerical columns have null values separately\n",
    "cat_null_columns = [col for col in cat_columns if df_dropped[col].isnull().any()]\n",
    "num_null_columns = [col for col in num_columns if df_dropped[col].isnull().any()]\n",
    "\n",
    "print(f\"Categorical columns with null values: {cat_null_columns}\")\n",
    "print(f\"Total number of categorical columns with null values: {len(cat_null_columns)}\")\n",
    "\n",
    "print(f\"\\nNumerical columns with null values: {num_null_columns}\")\n",
    "print(f\"Total number of numerical columns with null values: {len(num_null_columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values:\n",
    "* Mode for categorical variables\n",
    "* Mean for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in categorical columns with mode\n",
    "most_freq_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "for column in cat_null_columns:\n",
    "    df_dropped[column] = most_freq_imp.fit_transform(df_dropped[[column]]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that all the categorical missing values are filled\n",
    "df_dropped.columns[df_dropped.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in numerical columns with mean\n",
    "for column in num_null_columns:\n",
    "    df_dropped[column].fillna(df_dropped[column].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in the dataframe\n",
    "still_null = df_dropped.columns.isna().sum()\n",
    "print(f\"There are {still_null} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data distributions and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for all numerical variables\n",
    "fig = plt.figure(figsize=(14, 18))\n",
    "for i, col in enumerate(num_columns):\n",
    "    plt.subplot(10, 4, i + 1)\n",
    "    df_dropped[col].hist()\n",
    "    plt.title(col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histograms we can observe that some variables are distributed normal (symmetrical) whereas some are skewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bar charts for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar charts for categorical variables\n",
    "fig = plt.figure(figsize=(14, 24))\n",
    "for i, col in enumerate(cat_columns):\n",
    "    plt.subplot(10, 4, i + 1)\n",
    "    df_dropped[col].value_counts().plot(kind='bar')\n",
    "    plt.title(col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above bar plots, we can observe the frequencies of each category for all categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmap for correlation between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical variables\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(df_dropped[num_columns].corr(), annot=True, square=True, cmap='coolwarm', annot_kws={\"size\": 5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above heatmap of correlation between numerical variables, we can observe that the relationship ranges between variables from moderately negative (-0.4) to strongly positive (0.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box plots for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_dropped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for all numerical variables\n",
    "fig = plt.figure(figsize=(14, 18))\n",
    "for i, col in enumerate(num_columns):\n",
    "    plt.subplot(10, 4, i + 1)\n",
    "    sns.boxplot(df_clean[col])\n",
    "    plt.title(col)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above boxplots we can see that there are data that are beyond the inter quartile ranges. So, if we use IQR method to remove the outliers it will remove nearly half of the data (already tried). Also, with Z-score method with 3 standard deviation as threshold removes around 30% of the data (already tried as well). So, I checked the histograms and the data description and then figured out that the data points that seems like outliers in the box plots are little bit rare but possible and logical data points. So, instead of removing them I decided to keep all of them. The code for IQR and Z-score method I used is mentioned below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def remove_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter = (df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 * IQR)\n",
    "    return df.loc[filter]  \n",
    "\n",
    "for col in num_columns:\n",
    "    df_clean = remove_outliers(df_clean, col)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define a threshold\n",
    "threshold = 3\n",
    "\n",
    "# For each numeric column, if the zscore is greater than threshold, remove it\n",
    "for col in num_columns:\n",
    "    df_clean = df_clean[(np.abs(zscore(df_clean[col])) < threshold)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Categorical Variables (Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all categorical columns in the dataframe\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 37 categorical variables in total. Some of them have specific order or hierarchy in the categories, some of them do not inherent order in the categories and some of them have only two categories. So, based on that we need to identify the optimal encoder for that cateorical variable. With the help of data description, I figured out the types of categories (specific order or not) for each categorical variables and divided them into three different groups to encode them using three different encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables with specific order in the categories\n",
    "ord_enc_cols = ['LotShape', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "        'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'GarageQual', 'GarageCond']\n",
    "\n",
    "# variables without any order in the categories\n",
    "one_hot_enc_cols = ['MSZoning', 'Street', 'LandContour', 'Utilities',\n",
    "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
    "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "       'Exterior2nd', 'Foundation', 'Heating', 'Electrical', 'Functional', 'GarageType', \n",
    "       'GarageFinish', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
    "\n",
    "# variables with only 2 categories\n",
    "bin_enc_cols = ['CentralAir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoders\n",
    "ord_enc = OrdinalEncoder()\n",
    "bin_enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding variables with a specific order in the categories\n",
    "for col in ord_enc_cols:\n",
    "    df_clean[col] = ord_enc.fit_transform(df_clean[[col]])\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding variables without any order in the categories\n",
    "df_clean = pd.get_dummies(df_clean, columns=one_hot_enc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding binary variables\n",
    "for col in bin_enc_cols:\n",
    "    df_clean[col] = bin_enc.fit_transform(df_clean[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting True and False values to 1 and 0s\n",
    "df_clean = df_clean.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_clean.copy()\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection I tried Recursive Feature Elimination (RFE) method but as the dimensionality of the data is huge (203 variables) it was computationally expensive. So, I am using Random Forest Importance method which measures the importance of a feature bu calculating the total reduction in the crterion (Gini importance or Mean Decrease Impurity) brought by that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=1)\n",
    "model.fit(df_encoded.drop('SalePrice', axis=1), df_encoded['SalePrice'])\n",
    "\n",
    "# Get the importance of each feature\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Map feature importance values with their corresponding feature names\n",
    "feature_importance = pd.Series(importance, index=df_encoded.drop('SalePrice', axis=1).columns).sort_values(ascending=False)\n",
    "\n",
    "# Keeping features which have importance of more than 0.001\n",
    "selected_features = feature_importance[feature_importance > 0.001].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the selected features\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe with the selected features and the target vairable\n",
    "df_final = pd.concat([df_encoded[selected_features], df_encoded[['SalePrice']]], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset\n",
    "def split_dataset(df, target_var):\n",
    "    features = df.drop(target_var, axis = 1) # Features\n",
    "    target = df[target_var] # Target variable\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2) # Splitting the data\n",
    "    return X_train, X_test, y_train, y_test # Returning train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the df_final dataset\n",
    "X_train, X_test, y_train, y_test = split_dataset(df_final, \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going the train various models (Logistic Regression, Random Forest, XGBoost) and compare their performance on 'df_final' dataframe where categorical variables are encoded and features are selected using Random Forest feature importance method. \n",
    "\n",
    "Then I will train CatBoostRegressor where I will use the dataframe that I have before encoding the cateorical features and feature selection. CatBoostRegressor can be trained on categorical variables by specifying them while fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameters to be tuned\n",
    "param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Create a Logistic Regression estimator\n",
    "estimator_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_lr = GridSearchCV(estimator=estimator_lr, param_grid=param_grid_lr, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params_lr = grid_search_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters for LogisticRegression\n",
    "best_params_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new LogisticRegression with the best hyperparameters\n",
    "best_lr_model = LogisticRegression(**best_params_lr)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameters to be tuned\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth' : [i for i in range(6,12)]\n",
    "}\n",
    "\n",
    "# Create a Random Forest estimator\n",
    "estimator_rf = RandomForestRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_rf = GridSearchCV(estimator=estimator_rf, param_grid=param_grid_rf, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params_rf = grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters for RandomForestRegressor\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new RandomForestRegressor with the best hyperparameters\n",
    "best_rf_model = RandomForestRegressor(**best_params_rf)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameters to be tuned\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [7, 10],\n",
    "    'colsample_bytree': [0.5, 0.7, 1],\n",
    "    'gamma': [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Create a XGBoost estimator\n",
    "estimator_xgb = xgb.XGBRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search_xgb = GridSearchCV(estimator=estimator_xgb, param_grid=param_grid_xgb, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params_xgb = grid_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters for XGBRegressor\n",
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new XGBRegressor with the best hyperparameters\n",
    "best_xgb_model = xgb.XGBRegressor(**best_params_xgb)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on testing dataset with above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_pred = best_lr_model.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "xgb_pred = best_xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate performance of the model based on different metrices\n",
    "def eval_metrics(y_test, pred):\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "    # Calculate Root Mean Squared Error (RMSE)\n",
    "    rmse = sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "    # Calculate R^2 Score\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"Logistic Regression performance:\\n\")\n",
    "eval_metrics(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"Random Forest performance:\\n\")\n",
    "eval_metrics(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regression\n",
    "print(\"XGBoost performance:\\n\")\n",
    "eval_metrics(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CatBoostRegressor with categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am going to train a CatBoostRegressor model without encoding the cateorical variables and without doing feature selection. So I am using 'df_dropped' dataframe as it is the one before encoding categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print df_dropped dataframe\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe into training and testing\n",
    "X_train_01, X_test_01, y_train_01, y_test_01 = split_dataset(df_dropped, \"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the indices of categorical variables by excluding float and int data types\n",
    "categorical_features_indices = np.where((X_train_01.dtypes != np.float64) & (X_train_01.dtypes != np.int64))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters grid to tune\n",
    "param_grid_cat = {\n",
    "    'depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# CatBoost regressor\n",
    "estimator_cat = CatBoostRegressor(loss_function='RMSE', cat_features=categorical_features_indices)\n",
    "\n",
    "# Grid search\n",
    "grid_search_cat = GridSearchCV(estimator=estimator_cat, param_grid=param_grid_cat, cv=3, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fitting\n",
    "grid_search_cat.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_cat = grid_search_cat.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters for CatBoostRegressor\n",
    "best_params_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new CatBoostRegressor with the best hyperparameters\n",
    "best_cat_model = CatBoostRegressor(**best_params_cat, cat_features=categorical_features_indices)\n",
    "\n",
    "# Fit the model with the training data\n",
    "best_cat_model.fit(X_train_01, y_train_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on testing dataset\n",
    "cat_pred = best_cat_model.predict(X_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation of CatBoostRegressor\n",
    "print(\"XGBoost performance:\\n\")\n",
    "eval_metrics(y_test_01, cat_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
